{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.etree import ElementTree\n",
    "from collections import defaultdict\n",
    "from itertools import count, zip_longest, dropwhile\n",
    "from operator import itemgetter\n",
    "from Bio import Entrez\n",
    "import numpy as np\n",
    "import xmltodict\n",
    "import time\n",
    "from config import *\n",
    "import json\n",
    "import re\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BaseParser:\n",
    "    \"\"\"The base class for xml file parsing.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.xml_file_path = kwargs['xml_file_path']\n",
    "        self.json_file_path = kwargs.get('json_file_path')\n",
    "        self.citation_file_path = kwargs.get('citation_file_path')\n",
    "\n",
    "    def create_tree(self):\n",
    "        \"\"\"Create the tree object from xml file.\"\"\"\n",
    "    \n",
    "        with open(self.xml_file_path) as f:\n",
    "            tree = ElementTree.parse(f)\n",
    "            root = tree.getroot()\n",
    "        return root\n",
    "\n",
    "    def json_creator(self, result):\n",
    "        \"\"\"Create a json file from result.\"\"\"\n",
    "\n",
    "        with open(self.json_file_path, 'w') as f:\n",
    "            json.dump(result, f, indent=4, ensure_ascii=False)\n",
    "    \n",
    "    def extract_citations(self):\n",
    "        \"\"\"Extract citations from the end of the guideline file.\"\"\"\n",
    "\n",
    "        root = self.create_tree()\n",
    "        tree = root.iter('text')\n",
    "        regex1 = re.compile(r'^S\\.?\\d+[\\d,\\s.-]+$')\n",
    "        regex2 = re.compile(r'^(S\\d+[\\d,\\s.-]+)(.*)')\n",
    "        result = {}\n",
    "        pre = None\n",
    "        tree = dropwhile(\n",
    "            lambda x: ''.join(i.text for i in x.findall('b')).strip().lower() != 'references' and \n",
    "            \"\".join(x.itertext()).strip() != 'R E F E R E N C E S',\n",
    "            tree,\n",
    "        )\n",
    "        for node in tree:\n",
    "            if node.findall('b') or node.findall('i'):\n",
    "                continue\n",
    "            text = \"\".join(node.itertext()).strip()\n",
    "            if regex1.match(text):\n",
    "                result[text.strip('. ')] = ''\n",
    "                pre = text.strip('. ')\n",
    "            elif regex2.match(text):\n",
    "                text, rest = regex2.match(text).groups()\n",
    "                result[text.strip('. ')] = rest\n",
    "                pre = text.strip('. ')\n",
    "            elif pre:\n",
    "                result[pre] += ' ' + text\n",
    "        return result\n",
    "\n",
    "    \n",
    "    def create_json(self):\n",
    "        \"\"\"Create the final json file from extracted data.\"\"\"\n",
    "\n",
    "        charts = self.extract_charts()\n",
    "        self.json_creator(charts)\n",
    "\n",
    "        if self.citation_version == 1:\n",
    "            citations = self.extract_citations()\n",
    "        else:\n",
    "            citations = self.extract_citations2()\n",
    "\n",
    "        self.json_file_path = self.citation_file_path\n",
    "        self.json_creator(citations)\n",
    "\n",
    "\n",
    "\n",
    "class GuidelinesParser(BaseParser):\n",
    "    \"\"\"Parsing guideline files.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.counter = 0\n",
    "        self.citation_version = kwargs['citation_version']\n",
    "\n",
    "    def extract_charts(self):\n",
    "        \"\"\"Extract data from charts.\"\"\"\n",
    "        \n",
    "        result = defaultdict(dict)\n",
    "        root = self.create_tree()\n",
    "        tree = list(root.iter('text'))\n",
    "        regex = re.compile(r'.*?([S\\d,\\s.–-]+\\d)$')\n",
    "        # look at triples of nodes (COR, LOE and Recommendations)\n",
    "        for ind, (node1, node2, node3) in enumerate(zip(tree, tree[1:], tree[2:])):\n",
    "            try:\n",
    "                text1 = \"\".join(node1.itertext()).strip()\n",
    "                text2 = \"\".join(node2.itertext()).strip()\n",
    "                text3 = \"\".join(node3.itertext()).strip()\n",
    "                # Chart header\n",
    "                # if node2.text is None and\n",
    "                if re.match(r\"[Rr]ecommendation(?:s)? for\", text2):\n",
    "                    self.counter += 1\n",
    "                    result[self.counter]['title'] = text1\n",
    "                    cc = 0\n",
    "                    for i in tree[ind+2:]:\n",
    "                        cc += 1\n",
    "                        if cc > 5:\n",
    "                           text2 = None\n",
    "                           break\n",
    "                        tx = ' ' + \"\".join(i.itertext()).strip()\n",
    "                        if tx in {' COR', ' LOE', ' RECOMMENDATIONS'}:\n",
    "                            break\n",
    "                        text2 += tx \n",
    "                    if text2 is None:\n",
    "                        continue \n",
    "                    result[self.counter]['sub_title'] = text2\n",
    "                    result[self.counter]['rows'] = []\n",
    "                # Filter recommendations based on their COR and LOE\n",
    "                elif text1 in {'IIa', 'IIb'} and text2 in {'B-R', 'B-NR', 'C-LD'}:\n",
    "                    tx = ''\n",
    "                    for i in tree[ind + 3:]:\n",
    "                        tx = ' ' + \"\".join(i.itertext()).strip()\n",
    "\n",
    "                        text3 +=  tx\n",
    "                        if tx.endswith(').') or regex.match(tx):\n",
    "                            break\n",
    "                    result[self.counter]['rows'].append([\n",
    "                        text1,\n",
    "                        text2,\n",
    "                        text3.split(None, 1)[1]\n",
    "                    ])\n",
    "            except Exception as exc:\n",
    "                print(exc)\n",
    "        return result\n",
    "\n",
    "\n",
    "class DataSupParser(BaseParser):\n",
    "    \"\"\"Parse data supplement files.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # The required regex for extracting acronym, author and year.\n",
    "        self.ref_open = False\n",
    "        self.version = kwargs['version']\n",
    "        self.left_down = kwargs['left_down']\n",
    "        self.left_up = kwargs['left_up']\n",
    "        self.left_down_2 = kwargs.get('left_down_2', 0)\n",
    "        self.left_up_2 = kwargs.get('left_up_2', 0)\n",
    "        self.RFG = kwargs['row_filed_gap']\n",
    "        self.aay_regex = re.compile(\n",
    "                r'^([\\w\\s,-]+(?:et\\s*al\\.)?)?\\W+(\\d{4})?.*(?:\\(\\d+\\))?\\W+(\\d+)\\W*'\n",
    "            )\n",
    "    \n",
    "    def add_if_found(self, d, left, text, titles):\n",
    "        \"\"\"Add the text to the corresponding key if it found one,\n",
    "        otherwise create a new key.\"\"\"\n",
    "\n",
    "        try:\n",
    "            candidate_key = next(i for i in d['data'] if left - self.RFG < i < left + self.RFG)\n",
    "        except StopIteration:\n",
    "            d['data'][left] = text\n",
    "        else:\n",
    "            d['data'][candidate_key] += ' ' + text\n",
    "        finally:\n",
    "            d['titles'] |= titles\n",
    "        return d\n",
    "\n",
    "    def extract_data_sup(self):\n",
    "        \"\"\"Extract data from charts.\"\"\"\n",
    "\n",
    "        root = self.create_tree()\n",
    "        tree = root.iter('text')\n",
    "        result = {}\n",
    "        d = {'data':{}, 'titles':set(), 'references': []}\n",
    "        s = 0\n",
    "        pre = 0\n",
    "        # loop over the nodes\n",
    "        num_regex = re.compile(r'\\(\\d+\\)')\n",
    "        while True:\n",
    "            try:\n",
    "                node = next(tree)\n",
    "            except StopIteration:\n",
    "                break\n",
    "            # get the left attribute of the node.\n",
    "            left = int(node.get('left'))\n",
    "            # titles are bold phrases on each section followed by colon.\n",
    "            titles = {i.text.strip(' :') for i in node.findall('b') if i.text and i.text.strip()}\n",
    "            # iterate through all sub tags and join the texts together.\n",
    "            text = \"\".join(node.itertext()).strip().replace('\\u2013', '-')\n",
    "            if not text:\n",
    "                d['titles'] |= titles\n",
    "                continue\n",
    "            \n",
    "            if (\"Search Terms\" in text) or (\"Date of Search\" in text) or text.startswith('Study'):\n",
    "                continue\n",
    "            # We detect the start of a new block or row of the table if the left\n",
    "            # is between 80 and 100\n",
    "            if self.left_down < left < self.left_up:\n",
    "                # filter noise\n",
    "                if 3 < len(text) < 40:\n",
    "                    # if the previous node has also been a starting node\n",
    "                    # this means that we're dealing with a multiline starting\n",
    "                    # node\n",
    "                    if self.left_down < pre < self.left_up:\n",
    "                        d = self.add_if_found(d, left, text, titles)\n",
    "                    else:\n",
    "                        result[s] = d\n",
    "                        d = {'data': {left: text}, 'titles':titles, 'references': []}\n",
    "                        s += 1\n",
    "                else:\n",
    "                    continue\n",
    "            # if we're not at a starting node we should add the node to the \n",
    "            # dictionary.\n",
    "            elif left > self.left_up:\n",
    "                if self.version == 3 and self.ref_open:\n",
    "                    ref = self.check_if_reference(text)\n",
    "                    if ref:\n",
    "                        d['references'] += [i for i in ref if i]\n",
    "                elif len(text) < 100:\n",
    "                    d = self.add_if_found(d, left, text, titles)\n",
    "        \n",
    "            pre = left\n",
    "            pre_text = text\n",
    "\n",
    "        return result\n",
    "\n",
    "    def clean_sup_dict(self, result):\n",
    "        \"\"\"Add proper keys to the dict, remove noisy data and\n",
    "        categorize the content.\"\"\"\n",
    "\n",
    "        # remove empty items and fix endpoint titles\n",
    "        tmp = {\n",
    "            k : {'data':{\n",
    "                i:j for i, j in v['data'].items() if j.strip()\n",
    "            }, 'titles': [\n",
    "                        i + '° ' + 'endpoint' if i.isdigit() else i \n",
    "                            for i in (v['titles'] - {'endpoint'})\n",
    "                        ],\n",
    "            }for k,v in result.items() \n",
    "        }\n",
    "        \n",
    "        final = {}\n",
    "        # use proper keys for table fields based on the number of fields\n",
    "        for key,value in tmp.items():\n",
    "            if len(value['data']) == 5:\n",
    "                if self.version == 1:\n",
    "                    data = dict(zip(BASIC_KEYS_5, value['data'].values()))\n",
    "                else:\n",
    "                    data = dict(zip(BASIC_KEYS_5_3, value['data'].values()))\n",
    "            elif len(value['data']) == 6:\n",
    "                if self.version == 1:\n",
    "                    data = dict(zip(BASIC_KEYS_6, value['data'].values()))\n",
    "                else:\n",
    "                    data = dict(zip(BASIC_KEYS_6_2, value['data'].values()))\n",
    "            elif len(value['data']) == 7:\n",
    "                if self.version == 1:\n",
    "                    data = dict(zip(BASIC_KEYS_7, value['data'].values()))\n",
    "                else:\n",
    "                    data = dict(zip(BASIC_KEYS_7_2, value['data'].values()))\n",
    "            elif len(value['data']) == 8:\n",
    "                data = dict(zip(BASIC_KEYS_8, value['data'].values()))\n",
    "            elif len(value['data']) == 11:\n",
    "                data = dict(zip(BASIC_KEYS_11, value['data'].values()))\n",
    "            elif len(value['data']) == 13:\n",
    "                data = dict(zip(BASIC_KEYS_13, value['data'].values()))\n",
    "            elif len(value['data']) == 14:\n",
    "                data = dict(zip(BASIC_KEYS_14, value['data'].values()))\n",
    "            else:\n",
    "                continue\n",
    "\n",
    "            try:\n",
    "                aay = data.pop('acronym_author_year').replace('●', '')\n",
    "            except:\n",
    "                print(\"Key Error: acronym_author_year\")\n",
    "                pmid = 0\n",
    "                cleaned = {}\n",
    "            else:\n",
    "                regex = re.compile(\n",
    "                    r'{}\\s*'.format(r'|'.join([\n",
    "                        re.escape(i) for i in value['titles'] if i in aay and len(i)>1]))\n",
    "                    )\n",
    "                # remove noisy strings \n",
    "                acr = re.sub(\n",
    "                    r'Study\\s?Acronym\\s?;|\\s?Author\\s?;|\\s?Year\\s?Published',\n",
    "                    '',\n",
    "                    ' '.join([i.strip() for i in regex.findall(aay)])).strip()\n",
    "                if not acr:\n",
    "                    # print('No Acronym for', aay)\n",
    "                    acr = None\n",
    "                else:\n",
    "                    aay = aay.replace(acr, '').strip()\n",
    "                try:\n",
    "                    author, year, pmid = self.aay_regex.search(aay).groups()\n",
    "                except AttributeError as exc:\n",
    "                    print(exc, aay)\n",
    "                    continue\n",
    "                cleaned = {'acronym': acr,'author': author, 'year': year}\n",
    "            regex = re.compile(r'({})\\s*:'.format(r'|'.join([re.escape(i) for i in value['titles']])))           \n",
    "            d = defaultdict(str)\n",
    "\n",
    "            # categorize the content of each row in chart based on the bold titles\n",
    "            split_regex = re.compile(r'\\u25cf|\\u2022')\n",
    "\n",
    "            for k, v in data.items():\n",
    "                sp = regex.split(v)\n",
    "                if len(sp) == 1:\n",
    "                    d[k] = [i for i in split_regex.split(sp[0].strip()) if i.strip()]\n",
    "                    continue\n",
    "                while True:\n",
    "                    try:\n",
    "                        item = split_regex.sub('', sp.pop(0))\n",
    "                    except (IndexError, re.error) as exc:\n",
    "                        break\n",
    "                    else:\n",
    "                        if not item.strip():\n",
    "                            continue\n",
    "                        if item in value['titles']:\n",
    "                            try:\n",
    "                                d[item] = split_regex.sub('', sp.pop(0)).strip()\n",
    "                            except IndexError:\n",
    "                                d[item] = ''\n",
    "                        else:\n",
    "                            d[k] += item\n",
    "            cleaned.update(d)\n",
    "            cleaned['pmid'] = pmid\n",
    "            yield cleaned\n",
    "    \n",
    "    def create_json(self):\n",
    "        \"\"\"Create the final json file.\"\"\"\n",
    "\n",
    "        result = list(\n",
    "            self.clean_sup_dict(\n",
    "                    self.extract_data_sup()\n",
    "                )\n",
    "            )\n",
    "        # result = {k: {i:list(j) if isinstance(j, set) else j \n",
    "        #         for i,j in v.items()} for k,v in self.extract_data_sup().items()}\n",
    "        self.json_creator(result)\n",
    "\n",
    "\n",
    "class Merge:\n",
    "    \"\"\"Merge guidelines with its respective supplemented data.\"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.json_file_name = kwargs['json_file_name']\n",
    "        self.guidelines_path = kwargs['guidelines_path']\n",
    "        self.data_supplements_path = kwargs['data_supplements_path']\n",
    "        self.citations_path = kwargs['citations_path']\n",
    "        self.citation_version = kwargs['citation_version']\n",
    "\n",
    "    def load_guidelines(self):\n",
    "        \"\"\"Read guidelines json file.\"\"\"\n",
    "\n",
    "        return self.read_json(\n",
    "            self.guidelines_path\n",
    "        )\n",
    "    \n",
    "    def load_data_supplements(self):\n",
    "        \"\"\"Read data supplement json file.\"\"\"\n",
    "\n",
    "        return self.read_json(\n",
    "            self.data_supplements_path\n",
    "        )\n",
    "    \n",
    "    def load_citations(self):\n",
    "        \"\"\"Load citations json file.\"\"\"\n",
    "        \n",
    "        return self.read_json(\n",
    "            self.citations_path\n",
    "        )\n",
    "    \n",
    "    def read_json(self, path):\n",
    "        \"\"\"Load a json file.\"\"\"\n",
    "\n",
    "        with open(path) as f:\n",
    "            return json.load(f)\n",
    "\n",
    "    def run(self):\n",
    "        guidelines = self.load_guidelines()\n",
    "        datasup = self.load_data_supplements()\n",
    "        citations = self.load_citations()\n",
    "        final_result = {}\n",
    "        if self.citation_version == 1:\n",
    "            cit_regex = re.compile(r'\\(?([S\\d–,\\s.-]+)(?:[\\).\\s]+)?$')\n",
    "        else:\n",
    "            cit_regex = re.compile(r'\\(?([\\d–,\\s]+)(?:[\\).\\s]+)?$')\n",
    "        for key, value in guidelines.items():\n",
    "            try:\n",
    "                value['rows']\n",
    "            except KeyError:\n",
    "                # corrupted guideline\n",
    "                continue\n",
    "            for row in value['rows']:\n",
    "                temp_data = {}\n",
    "                recom = row[-1]\n",
    "                try:\n",
    "                    cit_string = cit_regex.search(recom).group(1).strip(' .')\n",
    "                except Exception as exc:\n",
    "                    print(recom, exc)\n",
    "                    continue\n",
    "\n",
    "                for cit in self.parse_citation(cit_string):\n",
    "                    try:\n",
    "                        pmids = self.find_PMID(\n",
    "                            citations[cit.strip()]\n",
    "                        )\n",
    "                    except KeyError:\n",
    "                        print(\"ERROR! \", cit.strip(), \"Is missing.\")\n",
    "                        continue\n",
    "                    print(cit)\n",
    "                    for p in pmids:\n",
    "                        try:\n",
    "                            d = [i for i in datasup if i['pmid'] == p]\n",
    "                            if len(d) == 1:\n",
    "                                res = d[0]\n",
    "                            else:\n",
    "                                res = defaultdict(set)\n",
    "                                for i in d:\n",
    "                                    for j,k in i.items():\n",
    "                                            res[j].add(tuple(k) if isinstance(k, list) else k)\n",
    "                                res = {k:v.pop() if len(v) == 1 else list(v) for k,v in res.items()}\n",
    "                            temp_data[p] = res\n",
    "                        except KeyError as exc:\n",
    "                            print(\"Key Error:\", exc)\n",
    "                        else:\n",
    "                            break\n",
    "                temp_data = {k: self.find_abstract(k) if not v else v for k, v in temp_data.items()}\n",
    "                row.append(temp_data)\n",
    "            final_result[key] = value\n",
    "        return final_result\n",
    "\n",
    "    def parse_citation(self, cit_string):\n",
    "        sp = [i.split('\\u2013') for i in cit_string.strip('() ').split(',')]\n",
    "        for cit in sp:\n",
    "            if len(cit) == 2:\n",
    "                if self.citation_version == 1:\n",
    "                    base, start = cit[0].split('-')\n",
    "                    start = int(start.strip())\n",
    "                    end = int(cit[1].split('-')[-1].strip())\n",
    "                    for i in range(start, end + 1):\n",
    "                        yield f\"{base.strip()}-{i}\"\n",
    "                else:\n",
    "                    start, end = map(int, cit)\n",
    "                    for i in range(start, end + 1):\n",
    "                        yield f\"{i}\"\n",
    "            else:\n",
    "                yield cit[0].replace(' ', '')\n",
    "\n",
    "    def find_PMID(self, citation_info):\n",
    "        \"\"\"Find the PMID using PubMed's API.\"\"\"\n",
    "\n",
    "        years = re.findall(r'\\W(\\d{4})\\W', citation_info)\n",
    "        # q = ','.join(citation_info.split(',')[:2] + [f'({i})' for i in years])\n",
    "        q = re.sub(r'[.;]', '|', citation_info)\n",
    "        Entrez.email = 'test@example.com'\n",
    "        try:\n",
    "            handle = Entrez.esearch(db='pubmed',                                                                                             \n",
    "                                    sort='relevance',                                                                                        \n",
    "                                    retmax='3',\n",
    "                                    retmode='xml', \n",
    "                                    term=q)\n",
    "            results = Entrez.read(handle)\n",
    "        except TimeoutError:\n",
    "            return\n",
    "        try:\n",
    "            return results['IdList']\n",
    "        except IndexError as exc:\n",
    "            print(exc, 'query: ', q)\n",
    "    \n",
    "    def find_abstract(self, pmid):\n",
    "        \"\"\"Find the abstract of the papers which don't have any data supplement.\"\"\"\n",
    "\n",
    "        Entrez.email = 'test@example.com'\n",
    "        handle = Entrez.efetch(db='pubmed', id=pmid, rettype=\"gb\", retmode=\"xml\")\n",
    "        f = handle.read()\n",
    "        try:\n",
    "            res = xmltodict.parse(f)\n",
    "            cit = res['PubmedArticleSet']['PubmedArticle']['MedlineCitation']\n",
    "            title = cit['Article']['ArticleTitle']\n",
    "            abc = cit['Article']['Abstract']['AbstractText']\n",
    "            if isinstance(abc, str):\n",
    "                abstract = abc\n",
    "            else:\n",
    "                abstract = '\\n'.join([i if isinstance(i, str) else i['#text'] for i in abc])\n",
    "            authors = [i.get('LastName','') + ' ' + i.get('ForeName', '') for i in cit['Article']['AuthorList']['Author']]\n",
    "            result = {\n",
    "                'title': title,\n",
    "                'authors': ', '.join(authors),\n",
    "                'abstract': abstract\n",
    "            }\n",
    "        except Exception as exc:\n",
    "            print(str(exc)[:50])\n",
    "            # raise Exception('Probably KeyError').with_traceback(e.__traceback__)\n",
    "            handle = Entrez.efetch(db='pubmed', id=pmid, rettype=\"gb\", retmode=\"text\")\n",
    "            return handle.read()\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def create_json(self):\n",
    "        \"\"\"Create the final json file.\"\"\"\n",
    "\n",
    "        result = self.run()\n",
    "        with open(self.json_file_name, 'w') as f:\n",
    "            json.dump(result, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    for item in SCRAPING:\n",
    "        directory = item['directory']\n",
    "        guideline_xml = item['guideline_xml']\n",
    "        datasup_xml = item['datasup_xml']\n",
    "        cit_version = item['cit_version']\n",
    "        left_down = item['left_down']\n",
    "        left_up = item['left_up']\n",
    "        dsup_version = item['dsup_version']\n",
    "        guideline_version = item['guideline_version']\n",
    "        row_filed_gap = item['row_filed_gap']\n",
    "        left_down_2 = item.get('left_down_2', 0)\n",
    "        left_up_2 = item.get('left_up_2', 0)\n",
    "        title_font = item.get('title_font', 0)\n",
    "        subtitle_font = item.get('subtitle_font', 0)\n",
    "        class_font = item.get('class_font', 0)\n",
    "        print(directory)\n",
    "\n",
    "        if guideline_version == 1:\n",
    "            GP = GuidelinesParser\n",
    "        elif guideline_version == 2:\n",
    "            GP = GuidelinesParser2\n",
    "        else:\n",
    "            GP = GuidelinesParser3\n",
    "\n",
    "        p = GP(\n",
    "            xml_file_path=f'{directory}/{guideline_xml}',\n",
    "            json_file_path=f'{directory}/guidelines.json',\n",
    "            citation_file_path=f'{directory}/citations.json',\n",
    "            citation_version=cit_version,\n",
    "            title_font=title_font,\n",
    "            subtitle_font=subtitle_font,\n",
    "            class_font=class_font)\n",
    "        p.create_json()\n",
    "\n",
    "        p = DataSupParser(\n",
    "            xml_file_path=f'{directory}/{datasup_xml}',\n",
    "            json_file_path=f'{directory}/datasup.json',\n",
    "            left_down=left_down,\n",
    "            left_up=left_up,\n",
    "            left_down_2=left_down_2,\n",
    "            left_up_2=left_up_2,\n",
    "            version=dsup_version,\n",
    "            row_filed_gap=row_filed_gap)\n",
    "        p.create_json()\n",
    "\n",
    "        M = Merge(\n",
    "            json_file_name=f'{directory}/merged_result.json',\n",
    "            data_supplements_path=f'{directory}/datasup.json',\n",
    "            guidelines_path=f'{directory}/guidelines.json',\n",
    "            citations_path=f'{directory}/citations.json',\n",
    "            citation_version=cit_version,\n",
    "        )\n",
    "        M.create_json()\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
